{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:43:26.229923Z",
     "start_time": "2024-06-22T11:43:25.633614Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from pymab.policies.thompson_sampling import ThompsonSamplingPolicy\n",
    "from pymab.policies.greedy import GreedyPolicy\n",
    "from pymab.game import Game"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:43:26.378505Z",
     "start_time": "2024-06-22T11:43:26.231010Z"
    }
   },
   "source": [
    "n_bandits = 3\n",
    "\n",
    "reward_distribution = 'bernoulli'\n",
    "\n",
    "thomson_sampling = ThompsonSamplingPolicy(n_bandits=n_bandits, reward_distribution=reward_distribution)\n",
    "greedy = GreedyPolicy(optimistic_initilization=1,\n",
    "                      n_bandits=n_bandits, reward_distribution=reward_distribution)\n",
    "\n",
    "# Define Q-values, which are the true values of the bandits\n",
    "#Q_values = np.array([0.1, 0.5, -0.2, 0.4, 0.7, 0.45, 0.3, 0.2, 0.05, -0.1])\n",
    "Q_values = [0.3, 0.7, 0.1]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== name bernoulli\n",
      "====== name <class 'pymab.reward_distribution.BernoulliRewardDistribution'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown reward distribution: <class 'pymab.reward_distribution.BernoulliRewardDistribution'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m n_bandits \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m      3\u001B[0m reward_distribution \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbernoulli\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 5\u001B[0m thomson_sampling \u001B[38;5;241m=\u001B[39m \u001B[43mThompsonSamplingPolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_bandits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_bandits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward_distribution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward_distribution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m greedy \u001B[38;5;241m=\u001B[39m GreedyPolicy(optimistic_initilization\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m      7\u001B[0m                       n_bandits\u001B[38;5;241m=\u001B[39mn_bandits, reward_distribution\u001B[38;5;241m=\u001B[39mreward_distribution)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Define Q-values, which are the true values of the bandits\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#Q_values = np.array([0.1, 0.5, -0.2, 0.4, 0.7, 0.45, 0.3, 0.2, 0.05, -0.1])\u001B[39;00m\n",
      "File \u001B[0;32m~/pymab/pymab/policies/thompson_sampling.py:284\u001B[0m, in \u001B[0;36mThompsonSamplingPolicy.__new__\u001B[0;34m(cls, n_bandits, variance, reward_distribution)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m    279\u001B[0m     n_bandits: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m    280\u001B[0m     variance: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m,\n\u001B[1;32m    281\u001B[0m     reward_distribution: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgaussian\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Policy:\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m reward_distribution \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbernoulli\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 284\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mBernoulliThompsonSamplingPolicy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_bandits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_bandits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvariance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvariance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreward_distribution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward_distribution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m reward_distribution \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgaussian\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m GaussianThompsonSamplingPolicy(\n\u001B[1;32m    291\u001B[0m             n_bandits\u001B[38;5;241m=\u001B[39mn_bandits,\n\u001B[1;32m    292\u001B[0m             variance\u001B[38;5;241m=\u001B[39mvariance,\n\u001B[1;32m    293\u001B[0m             reward_distribution\u001B[38;5;241m=\u001B[39mreward_distribution,\n\u001B[1;32m    294\u001B[0m         )\n",
      "File \u001B[0;32m~/pymab/pymab/policies/thompson_sampling.py:59\u001B[0m, in \u001B[0;36mBernoulliThompsonSamplingPolicy.__init__\u001B[0;34m(self, n_bandits, optimistic_initilization, variance, reward_distribution)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     54\u001B[0m     n_bandits: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m     reward_distribution: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgaussian\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     58\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_bandits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimistic_initilization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward_distribution\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimes_success \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_bandits)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimes_failure \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_bandits)\n",
      "File \u001B[0;32m~/pymab/pymab/policies/policy.py:47\u001B[0m, in \u001B[0;36mPolicy.__init__\u001B[0;34m(self, n_bandits, optimistic_initilization, variance, reward_distribution)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariance \u001B[38;5;241m=\u001B[39m variance\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_distribution\u001B[49m \u001B[38;5;241m=\u001B[39m reward_distribution\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimes_selected \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_bandits)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions_estimated_reward \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfull(\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_bandits, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimistic_initilization, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m\n\u001B[1;32m     51\u001B[0m )\n",
      "File \u001B[0;32m~/pymab/pymab/policies/policy.py:67\u001B[0m, in \u001B[0;36mPolicy.reward_distribution\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m distributions:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown reward distribution: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_distribution\u001B[49m \u001B[38;5;241m=\u001B[39m distributions[name]\n",
      "File \u001B[0;32m~/pymab/pymab/policies/policy.py:66\u001B[0m, in \u001B[0;36mPolicy.reward_distribution\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     60\u001B[0m distributions \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgaussian\u001B[39m\u001B[38;5;124m\"\u001B[39m: GaussianRewardDistribution,\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbernoulli\u001B[39m\u001B[38;5;124m\"\u001B[39m: BernoulliRewardDistribution,\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m: UniformRewardDistribution,\n\u001B[1;32m     64\u001B[0m }\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m distributions:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown reward distribution: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_distribution \u001B[38;5;241m=\u001B[39m distributions[name]\n",
      "\u001B[0;31mValueError\u001B[0m: Unknown reward distribution: <class 'pymab.reward_distribution.BernoulliRewardDistribution'>"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:43:26.379532Z",
     "start_time": "2024-06-22T11:43:26.379473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the game\n",
    "game = Game(n_episodes=100, \n",
    "            n_steps=1000, \n",
    "            policies=[\n",
    "                    greedy,\n",
    "                    thomson_sampling\n",
    "                ], \n",
    "            n_bandits=n_bandits,\n",
    "            Q_values=Q_values,\n",
    "            is_stationary=False\n",
    "            )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for policy in game.policies:\n",
    "    policy.Q_values = game.Q_values\n",
    "    policy.plot_distribution()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the game\n",
    "game.game_loop()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for policy in game.policies:\n",
    "    policy.plot_distribution()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the results\n",
    "game.plot_average_reward_by_step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "game.plot_average_reward_by_step_smoothed()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "game.plot_total_reward_by_step()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "game.plot_rate_optimal_actions_by_step()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "game.plot_cumulative_regret_by_step()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
