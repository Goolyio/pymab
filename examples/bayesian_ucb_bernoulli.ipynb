{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:41:52.051024Z",
     "start_time": "2024-08-02T15:41:49.921296Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "from pymab.policies.ucb import UCBPolicy\n",
    "from pymab.policies.bayesian_ucb import BayesianUCBPolicy\n",
    "from pymab.game import Game"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pymc3:The version of PyMC you are using is very outdated.\n",
      "\n",
      "Please upgrade to the latest version of PyMC https://www.pymc.io/projects/docs/en/stable/installation.html\n",
      "\n",
      "Also notice that PyMC3 has been renamed to PyMC.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:41:52.054825Z",
     "start_time": "2024-08-02T15:41:52.052101Z"
    }
   },
   "source": [
    "# Define Q-values, which are the true values of the bandits\n",
    "Q_values = [-0.1, 0.8, 0.3, 0.4, -0.9, 0.2, 0.25, 0.6, 0.5, -0.3]\n",
    "n_bandits = 10\n",
    "\n",
    "reward_distribution = 'bernoulli'\n",
    "\n",
    "ucb_policy_0 = UCBPolicy(n_bandits=n_bandits,\n",
    "                      c=0, reward_distribution=reward_distribution)\n",
    "\n",
    "ucb_policy_1 = UCBPolicy(n_bandits=n_bandits,\n",
    "                      c=1, reward_distribution=reward_distribution)\n",
    "\n",
    "ucb_policy_2 = UCBPolicy(n_bandits=n_bandits,\n",
    "                      c=2, reward_distribution=reward_distribution)\n",
    "\n",
    "bayesian_ucb = BayesianUCBPolicy(n_bandits=n_bandits, reward_distribution=reward_distribution)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:41:52.057725Z",
     "start_time": "2024-08-02T15:41:52.055549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the game\n",
    "game = Game(n_episodes=2000, \n",
    "            n_steps=1000, \n",
    "            Q_values=Q_values,\n",
    "            policies=[ucb_policy_0,\n",
    "                    ucb_policy_1,\n",
    "                    ucb_policy_2,\n",
    "                    bayesian_ucb\n",
    "                ], \n",
    "            n_bandits=n_bandits,\n",
    "            )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:41:52.276857Z",
     "start_time": "2024-08-02T15:41:52.059009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the game\n",
    "game.game_loop()"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "p < 0, p > 1 or p is NaN",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the game\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mgame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgame_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pymab/pymab/game.py:125\u001B[0m, in \u001B[0;36mGame.game_loop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps):\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;66;03m# print(\"\\n\\n========= Episode: \", episode, \"Step: \", step)\u001B[39;00m\n\u001B[1;32m    124\u001B[0m     context \u001B[38;5;241m=\u001B[39m policy\u001B[38;5;241m.\u001B[39mcontext_func()\n\u001B[0;32m--> 125\u001B[0m     action, reward \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards_by_policy[episode, step, policy_index] \u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions_selected_by_policy[episode, step, policy_index] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    128\u001B[0m         action\n\u001B[1;32m    129\u001B[0m     )\n",
      "File \u001B[0;32m~/pymab/pymab/policies/ucb.py:62\u001B[0m, in \u001B[0;36mUCBPolicy.select_action\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m     ucb_values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\n\u001B[1;32m     58\u001B[0m         [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_ucb_value(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_bandits)]\n\u001B[1;32m     59\u001B[0m     )\n\u001B[1;32m     60\u001B[0m     chosen_action_index \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(ucb_values)\n\u001B[0;32m---> 62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chosen_action_index, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchosen_action_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pymab/pymab/policies/policy.py:82\u001B[0m, in \u001B[0;36mPolicy._update\u001B[0;34m(self, chosen_action_index, *args, **kwargs)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, chosen_action_index: \u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 82\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_actual_reward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchosen_action_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimes_selected[chosen_action_index] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/pymab/pymab/policies/policy.py:76\u001B[0m, in \u001B[0;36mPolicy._get_actual_reward\u001B[0;34m(self, action_index)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_actual_reward\u001B[39m(\u001B[38;5;28mself\u001B[39m, action_index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_distribution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_reward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQ_values\u001B[49m\u001B[43m[\u001B[49m\u001B[43maction_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvariance\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pymab/pymab/reward_distribution.py:110\u001B[0m, in \u001B[0;36mBernoulliRewardDistribution.get_reward\u001B[0;34m(q_value, variance)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_reward\u001B[39m(q_value: \u001B[38;5;28mfloat\u001B[39m, variance: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     98\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get a reward sampled from a Bernoulli distribution.\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;124;03m        If q_value = 0.3, approximately 70% of the sampled values will be 1 (successes) and 30% will be 0 (unsuccesses).\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinomial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32mmtrand.pyx:3415\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.binomial\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_common.pyx:412\u001B[0m, in \u001B[0;36mnumpy.random._common.check_constraint\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: p < 0, p > 1 or p is NaN"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for policy in game.policies:\n",
    "    policy.plot_distribution()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the results\n",
    "game.plot_average_reward_by_step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "game.plot_average_reward_by_step_smoothed()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "* **Greedy policy without optimistic initialization** behaves similarly to **UCB with c = 0**. This is because, if c = 0, there is no encouragement for exploration, meaning that after initialization, it will always choose the action with the highest estimated reward."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "game.plot_rate_optimal_actions_by_step()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
