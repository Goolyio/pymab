{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T21:24:53.411304Z",
     "start_time": "2024-10-08T21:24:52.091568Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymab.policies.ucb import StationaryUCBPolicy\n",
    "from pymab.game import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T21:24:53.581225Z",
     "start_time": "2024-10-08T21:24:53.412468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pymc3:The version of PyMC you are using is very outdated.\n",
      "\n",
      "Please upgrade to the latest version of PyMC https://www.pymc.io/projects/docs/en/stable/installation.html\n",
      "\n",
      "Also notice that PyMC3 has been renamed to PyMC.\n"
     ]
    }
   ],
   "source": [
    "from pymab.policies.bayesian_ucb import BayesianUCBPolicy\n",
    "\n",
    "# Define Q-values, which are the true values of the bandits\n",
    "# For bernoulli distribution, the values should always be between 0 and 1.\n",
    "Q_values = [0.1, 0.8, 0.3, 0.4, 0.9, 0.2, 0.25, 0.6, 0.5, 0.35]\n",
    "n_bandits = 10\n",
    "\n",
    "reward_distribution = 'bernoulli'\n",
    "\n",
    "ucb_policy_0 = StationaryUCBPolicy(n_bandits=n_bandits,\n",
    "                      c=0, reward_distribution=reward_distribution)\n",
    "\n",
    "ucb_policy_1 = StationaryUCBPolicy(n_bandits=n_bandits,\n",
    "                      c=1, reward_distribution=reward_distribution)\n",
    "\n",
    "ucb_policy_2 = StationaryUCBPolicy(n_bandits=n_bandits,\n",
    "                      c=2, reward_distribution=reward_distribution)\n",
    "\n",
    "bayesian_ucb = BayesianUCBPolicy(n_bandits=n_bandits, reward_distribution=reward_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T21:25:04.428972Z",
     "start_time": "2024-10-08T21:25:04.408852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the game\n",
    "game = Game(n_episodes=2000, \n",
    "            n_steps=1000, \n",
    "            Q_values=Q_values,\n",
    "            policies=[ucb_policy_0,\n",
    "                    ucb_policy_1,\n",
    "                    ucb_policy_2,\n",
    "                    bayesian_ucb\n",
    "                ], \n",
    "            n_bandits=n_bandits,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T21:25:08.647715Z",
     "start_time": "2024-10-08T21:25:08.627647Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "WARNING:pymc3:The acceptance probability does not match the target. It is 0.8837097769922425, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "WARNING:pymc3:The acceptance probability does not match the target. It is 0.8763403989079345, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "WARNING:pymc3:The acceptance probability does not match the target. It is 0.9028652610993134, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "/Users/dlopes/pymab/.venv/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n"
     ]
    }
   ],
   "source": [
    "# Run the game\n",
    "game.game_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for policy in game.policies:\n",
    "    policy.plot_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "game.plot_average_reward_by_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.plot_average_reward_by_step_smoothed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Greedy policy without optimistic initialization** behaves similarly to **UCB with c = 0**. This is because, if c = 0, there is no encouragement for exploration, meaning that after initialization, it will always choose the action with the highest estimated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.plot_rate_optimal_actions_by_step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
